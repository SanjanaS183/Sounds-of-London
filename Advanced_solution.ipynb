{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gJVADYvuHq3x",
        "kp1Kgp5OkdC-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjanaS183/Sounds-of-London/blob/main/Advanced_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gjrwd7Cxez5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259a8925-b182-4579-9e39-92e4131a4d64"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os, sys, re, pickle, glob\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import IPython.display as ipd\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD2rP-8AzWS4"
      },
      "source": [
        "# Understanding our dataset sample\n",
        "\n",
        "Let's unzip the sample data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOPTdwA9zeyz"
      },
      "source": [
        "directory_to_extract_to = '/content/drive/MyDrive/Data/MLEndLS/Files/'\n",
        "zip_path = '/content/drive/MyDrive/Data/MLEndLS/MLEndLS_2.zip'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now check how many audio files we have:"
      ],
      "metadata": {
        "id": "rdP6Mk0PriRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_path = '/content/drive/MyDrive/Data/MLEndLS/*.wav'\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "files = glob.glob(sample_path)\n",
        "len(files)"
      ],
      "metadata": {
        "id": "BYrpz_QkLAp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aef278c-3cca-4e28-e9ca-21fa3e74d39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0OeDvt7zedb"
      },
      "source": [
        "This figure (2500) corresponds to the number of **items** or **samples** in our dataset. Let's listen to some random audio files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcqXgKzgzmwG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "3772e28e-c3a0-45d0-b67e-a267702bf970"
      },
      "source": [
        "for _ in range(5):\n",
        "  n = np.random.randint(98)\n",
        "  display(ipd.Audio(files[n]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ab366e632ab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLENDLS_df = pd.read_csv('./MLEndLS.csv').set_index('file_id')\n",
        "MLENDLS_df"
      ],
      "metadata": {
        "id": "BxXrmlDt1DV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHa-FqEg2Gyu"
      },
      "source": [
        "for file in files:\n",
        "  print(file.split('/')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_gender(audio_signal)\n",
        " pitch, magnitude = librosa.core.piptrack(y = audio_signal, sr =16000)\n",
        " max_pitch = pitch[np.where(magnitude == np.max(magnitude))]\n",
        " if max_pitch >=85 and max_pitch<=180:\n",
        "   return 'Male'\n",
        " elif max_pitch>=165 and max_pitch<=180:\n",
        "   return 'Female'\n",
        " else:\n",
        "  return 'Unknown'"
      ],
      "metadata": {
        "id": "4R6Jj-gNK3dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender = detect_gender(files)"
      ],
      "metadata": {
        "id": "x2mwP7lQrzVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJVADYvuHq3x"
      },
      "source": [
        "# Feature extraction : Pitch\n",
        "\n",
        "Audio files are complex data types. Specifically they are **discrete signals** or **time series**, consisting of values on a 1D temporal grid. These values are known as *samples* themselves, which might be a bit confusing, as we have used this term to refer to the *items* in our dataset. The **sampling frequency** is the rate at which samples in an audio file are produced. For instance a sampling frequency of 5HZ indicates that 5 produce 5 samples per second, or 1 sample every 0.2 s.\n",
        "\n",
        "Let's plot one of our audio signals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU7cSdySHx8P"
      },
      "source": [
        "n=0\n",
        "fs = None # Sampling frequency\n",
        "x, fs = librosa.load(files[n],sr=fs)\n",
        "t = np.arange(len(x))/fs\n",
        "plt.plot(t,x)\n",
        "plt.xlabel('time (sec)')\n",
        "plt.ylabel('amplitude')\n",
        "plt.show()\n",
        "display(ipd.Audio(files[n]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVndEmbJIR5-"
      },
      "source": [
        "Can you tell whether you are listening to an indoors or outdoors location? Does it agree with the values shown in the ``` MLENDLS_df ``` dataframe? Let's check it:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4cSPJxnT80j"
      },
      "source": [
        "MLENDLS_df.loc[files[n].split('/')[-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Weh65CY7T9da"
      },
      "source": [
        "Note that we are using the name of the audio file as the index in the Pandas DataFrame. By changing the value of `n` in the previous cell, you can listen to other examples. If you are doing this during one of our lab sessions, please make sure that your mic is muted!\n",
        "\n",
        "Exactly, how complex is an audio signal? Let's start by looking at the number of samples (i.e. time series samples) in one of our audio files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSPjIW9JZWv"
      },
      "source": [
        "n=0\n",
        "x, fs = librosa.load(files[n],sr=fs)\n",
        "print('This audio signal has', len(x), 'samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrNs96siJggo"
      },
      "source": [
        "If we are using a raw audio signal as the input of a machine learning model, we will be operating in a predictor space consisting of hundreds of thousands of dimensions. Compare this figure with the number of items (i.e. recordings) that we have. Do we have enough samples to train a model that takes one of these audio signals as an input?\n",
        "\n",
        "One approach to deal with this huge dimensionality is to extract a few features from our signals and use these features as predictors instead. In this notebook we will use four audio features, namely:\n",
        "\n",
        "\n",
        "1.   Power.\n",
        "2.   Pitch - mean.\n",
        "3.   Pitch - standard deviation.\n",
        "4.   Fraction of voiced region.\n",
        "\n",
        "In the next cell, we define a new function that gets the pitch of an audio signal (do not worry if you do not know what it is, but feel free to read about it!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiXYO1tdJgIs"
      },
      "source": [
        "def getPitch(x,fs,winLen=0.02):\n",
        "  #winLen = 0.02\n",
        "  p = winLen*fs\n",
        "  frame_length = int(2**int(p-1).bit_length())\n",
        "  hop_length = frame_length//2\n",
        "  f0, voiced_flag, voiced_probs = librosa.pyin(y=x, fmin=80, fmax=450, sr=fs,\n",
        "                                                 frame_length=frame_length,hop_length=hop_length)\n",
        "  return f0,voiced_flag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCRRi_NFM3kw"
      },
      "source": [
        "Let's consider the problem of determining whether the filming spot is indoors or outdoors. Then next cell defines a function that takes a collection of audio files together with a CSV file and creates a NumPy array containing the 4 audio features used as predictors (`X`) and a binary label (`y`) that indicates whether the recording is indoors (`y=1`) or outdoors (`y=0`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEMwPrVmMHZa"
      },
      "source": [
        "def getXy(files,labels_file, scale_audio=False, onlySingleDigit=False):\n",
        "  labels= ['campus', 'british']\n",
        "  X,y,z =[],[],[]\n",
        "  for file in tqdm(files):\n",
        "    fileID = file.split('/')[-1]\n",
        "    file_name = file.split('/')[-1]\n",
        "    #i = labels_file.loc[fileID]['in_out']=='indoor'\n",
        "    if(labels_file.loc[fileID]['area']==labels[0]):\n",
        "      yj = 0 # label 0 if file is from campus\n",
        "    else:\n",
        "      yj = 1\n",
        "    print(file_name)\n",
        "    fs = None # if None, fs would be 22050\n",
        "    x, fs = librosa.load(file,sr=fs)\n",
        "    if scale_audio: x = x/np.max(np.abs(x))\n",
        "    f0, voiced_flag = getPitch(x,fs,winLen=0.02)\n",
        "\n",
        "    power = np.sum(x**2)/len(x)\n",
        "    pitch_mean = np.nanmean(f0) if np.mean(np.isnan(f0))<1 else 0\n",
        "    pitch_std  = np.nanstd(f0) if np.mean(np.isnan(f0))<1 else 0\n",
        "    voiced_fr = np.mean(voiced_flag)\n",
        "\n",
        "    xi = [power,pitch_mean,pitch_std,voiced_fr]\n",
        "    X.append(xi)\n",
        "    #y.append(yi)\n",
        "    z.append(yj)\n",
        "\n",
        "  return np.array(X),np.array(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQdqy2ksNCD7"
      },
      "source": [
        "Let's apply `getXy` to the subsample and obtain the NumPy predictor array (`X`) and a binary label (`y`). This could take a while, as we are processing each of the 100 audio signals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkjX_Zu9NNUs"
      },
      "source": [
        "X,z= getXy(files, labels_file=MLENDLS_df, scale_audio=True, onlySingleDigit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOA190Z0kSVq"
      },
      "source": [
        "The next cell shows the shape of `X` and `y` and prints the labels vector `y`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDtFySakTeP"
      },
      "source": [
        "print('The shape of X is', X.shape)\n",
        "print('The shape of y is', z.shape)\n",
        "print('The labels vector is', z)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVvbOCDBZbQx"
      },
      "source": [
        "As you can see, we have 100 items consisting of 4 features (stored in `X`) and one binary label (stored in `y`). Is our dataset balanced? Let's have a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW_grfnuYmjD"
      },
      "source": [
        "print(' The number of indoor recordings is ', np.count_nonzero(z))\n",
        "print(' The number of outdoor recordings is ', z.size - np.count_nonzero(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler() # define standard scaler\n",
        "scaled = scaler.fit_transform(X) # transform data\n",
        "\n",
        "#converting the scaled features into a pandas data frame\n",
        "allsong_feature_combine= pd.DataFrame(scaled)\n",
        "allsong_feature_combine['area']= z\n",
        "\n",
        "#saving the features into a csv for future reference\n",
        "allsong_feature_combine.to_csv('/content/allsong_feature_combine.csv')\n"
      ],
      "metadata": {
        "id": "ZfeFsy6p9pxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allsongs_df_scaled= pd.read_csv('/content/allsong_feature_combine.csv').drop(columns={'Unnamed: 0'})"
      ],
      "metadata": {
        "id": "lUXy6-it-4jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allsongs_df_scaled"
      ],
      "metadata": {
        "id": "8qz4Bg-IFXrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=allsongs_df_scaled.iloc[:,0:4].values # all the features\n",
        "z=allsongs_df_scaled['area'].values"
      ],
      "metadata": {
        "id": "ut8xlMqzFtr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X\n",
        "z"
      ],
      "metadata": {
        "id": "UokvwtuuLAl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp1Kgp5OkdC-"
      },
      "source": [
        "# Modeling: Support Vector Machines\n",
        "\n",
        "Let's build a support vector machine (SVM) model for the predictive task of identifying the type of filming spot (indoors/outdoors) using the dataset that we have just created.\n",
        "\n",
        "We will use the SVM method provided by scikit-learn and will split the dataset defined by X and y into a training set and a validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv0rBpWFkoe6"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,z,test_size=0.3)\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqPeEFEqk_cS"
      },
      "source": [
        "Can you identify the number of items in the training and validation sets?\n",
        "\n",
        "Let's now fit an SVM model and print both the training accuracty and validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZYHghqhlDeM"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = svm.SVC(C=10)\n",
        "\n",
        "parameters = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "\n",
        "grid_search = GridSearchCV(estimator= model,\n",
        "                          param_grid = parameters, scoring = 'accuracy',cv = 10)\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "yt_p1 = grid_search.predict(X_train)\n",
        "yv_p1 = grid_search.predict(X_val)\n",
        "\n",
        "print('Training Accuracy', np.mean(yt_p1==y_train))\n",
        "print('Validation  Accuracy', np.mean(yv_p1==y_val))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFr71A-blKsN"
      },
      "source": [
        "Compare the training and validation accuracies. Is our model overfitting, underfitting, performing well? What do you think the accuracy of a random classifier would be?\n",
        "\n",
        "Let's normalise the predictors, to see if the performance improves.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJGHrtdWlLre"
      },
      "source": [
        "mean = X_train.mean(0)\n",
        "sd =  X_train.std(0)\n",
        "\n",
        "X_train = (X_train-mean)/sd\n",
        "X_val  = (X_val-mean)/sd\n",
        "\n",
        "model = svm.SVC(C=10)\n",
        "\n",
        "parameters = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "\n",
        "grid_search = GridSearchCV(estimator= model,\n",
        "                          param_grid = parameters, scoring = 'accuracy',cv = 10)\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "yt_p1 = grid_search.predict(X_train)\n",
        "yv_p1 = grid_search.predict(X_val)\n",
        "\n",
        "print('Training Accuracy', np.mean(yt_p1==y_train))\n",
        "print('Validation  Accuracy', np.mean(yv_p1==y_val))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import librosa\n",
        "import librosa.display\n",
        "#audio_path = '/content/drive/MyDrive/Data/MLEndLS/sample/'\n",
        "x, fs = librosa.load(files[n],sr=fs)\n",
        "mfcc = librosa.feature.mfcc(y=x, sr=fs, n_mfcc=40)\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mfcc, x_axis='time')\n",
        "plt.colorbar()\n",
        "plt.title('MFCC')\n",
        "plt.tight_layout()\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "-WXSlBo3Vo4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getXy(files,labels_file, scale_audio=False, onlySingleDigit=False):\n",
        "\n",
        "  X=[]\n",
        "  for file in tqdm(files):\n",
        "    fileID = file.split('/')[-1]\n",
        "    file_name = file.split('/')[-1]\n",
        "  fs = None\n",
        "  x, fs = librosa.load(file,sr=fs)\n",
        "  if scale_audio: x = x/np.max(np.abs(x))\n",
        "  f0, voiced_flag = getPitch(files[n],fs,winLen=0.02)\n",
        "  pitch_mean = np.nanmean(f0) if np.mean(np.isnan(f0))<1 else 0\n",
        "  u = np.array(pitch_mean)\n",
        "  return u"
      ],
      "metadata": {
        "id": "Kz5xTdMRfbfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= getXy(files, labels_file=MLENDLS_df, scale_audio=True, onlySingleDigit=True)"
      ],
      "metadata": {
        "id": "fSW3oEFlgU9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "allsongs_df_scaled[allsongs_df_scaled['label']==True].describe() #for humming files\n"
      ],
      "metadata": {
        "id": "woJTeu9BV7S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allsongs_df_scaled[allsongs_df_scaled['label']==False].describe()"
      ],
      "metadata": {
        "id": "E6oXWwnyV-sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clust= allsongs_df_scaled[allsongs_df_scaled['label']==True]"
      ],
      "metadata": {
        "id": "FZ7LuSpgaQ0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc= df_clust.iloc[:,0:4]"
      ],
      "metadata": {
        "id": "q85GjN6mag3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
        "cluster.fit_predict(mfcc)"
      ],
      "metadata": {
        "id": "prFUx0IQaqpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(df_clust['0'], df_clust['3'], c=cluster.labels_)"
      ],
      "metadata": {
        "id": "SGbvJm9WbMNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMHpZlB5lPr_"
      },
      "source": [
        "Do you think we have obtained a better solution?\n",
        "\n",
        "What machine learning pipeline have we implemented for each solution? Feel free to try other machine learning models available in scikit, it's very easy!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original parameters :\n",
        "training accuracy (TA) - 0.928\n",
        "validation accuracy(VA) - 0.6\n",
        "\n",
        "Test set - 0.4:\n",
        "TA- 0.883\n",
        "VA- 0.65\n",
        "\n",
        "Using grid search:\n",
        "TA- 0.9\n",
        "VA- 0.7\n",
        "\n",
        "Using Random forest:\n",
        "training accuracy became 1?\n",
        "TA - 1.0\n",
        "VA- 0.525\n",
        "Overfitting\n",
        "\n",
        "SVM-\n",
        "c = 10\n",
        "TA- 0.816\n",
        "VA- 0.6\n",
        "\n",
        "\n",
        "TA - 0.98\n",
        "VA - 0.6"
      ],
      "metadata": {
        "id": "i4iF2IUlozG6"
      }
    }
  ]
}